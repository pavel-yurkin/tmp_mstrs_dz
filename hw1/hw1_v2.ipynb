{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "en_emb = KeyedVectors.load_word2vec_format(\"cc.en.300.vec\")\n",
    "fr_emb = KeyedVectors.load_word2vec_format(\"cc.fr.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300,), array([-0.0522,  0.0364, -0.1252,  0.0053,  0.0382], dtype=float32))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "august_embedding = en_emb[\"august\"]\n",
    "august_embedding.shape, august_embedding[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('august', 1.0000001192092896),\n",
       " ('september', 0.8252838253974915),\n",
       " ('october', 0.8111194372177124),\n",
       " ('june', 0.8050148487091064),\n",
       " ('july', 0.7970553636550903),\n",
       " ('november', 0.7883636355400085),\n",
       " ('february', 0.7831972241401672),\n",
       " ('december', 0.7824541330337524),\n",
       " ('january', 0.774315595626831),\n",
       " ('april', 0.7621644139289856)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_emb.most_similar([august_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('august', 1.0000001192092896),\n",
       " ('september', 0.8252838253974915),\n",
       " ('october', 0.8111194372177124)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_emb.most_similar([august_embedding], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_emb[[\"august\", \"september\"]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aout', 1.0),\n",
       " ('Aout', 0.8249963521957397),\n",
       " ('juillet', 0.8109882473945618),\n",
       " ('fevrier', 0.8072444200515747),\n",
       " ('septembre', 0.7838519811630249),\n",
       " ('août', 0.779176652431488),\n",
       " ('juin', 0.7692081332206726),\n",
       " ('octobre', 0.7597455382347107),\n",
       " ('decembre', 0.7595792412757874),\n",
       " ('avril', 0.7390779256820679)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_emb.most_similar([fr_emb[\"aout\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2003Pays', 0.23082853853702545),\n",
       " ('Montsoriu', 0.22505582869052887),\n",
       " ('2015Pays', 0.22218404710292816),\n",
       " ('2013Genre', 0.2095685452222824),\n",
       " ('AdiCloud', 0.20186512172222137),\n",
       " ('Bagua', 0.20061466097831726),\n",
       " ('2003Paysans', 0.2001495510339737),\n",
       " ('ValenceLa', 0.2001475840806961),\n",
       " ('Luddites', 0.19998176395893097),\n",
       " ('Guadalquivir', 0.1987551599740982)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_emb.most_similar([en_emb[\"august\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_pairs(filename):\n",
    "    en_fr_pairs = []\n",
    "    en_vectors = []\n",
    "    fr_vectors = []\n",
    "    with open(filename, \"r\") as inpf:\n",
    "        for line in inpf:\n",
    "            en, fr = line.rstrip().split(\" \")\n",
    "            if en not in en_emb or fr not in fr_emb:\n",
    "                continue\n",
    "            en_fr_pairs.append((en, fr))\n",
    "            en_vectors.append(en_emb[en])\n",
    "            fr_vectors.append(fr_emb[fr])\n",
    "    return en_fr_pairs, np.array(en_vectors), np.array(fr_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_fr_train, X_train, Y_train = load_word_pairs(\"en-fr.train.txt\")\n",
    "en_fr_test, X_test, Y_test = load_word_pairs(\"en-fr.test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('torpedo', 'torpille'),\n",
       " ('torpedo', 'torpilles'),\n",
       " ('giovanni', 'giovanni'),\n",
       " ('chat', 'discuter'),\n",
       " ('chat', 'discussion')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_fr_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('which', 'laquelle'),\n",
       " ('which', 'lequel'),\n",
       " ('also', 'aussi'),\n",
       " ('also', 'egalement'),\n",
       " ('but', 'mais'),\n",
       " ('have', 'avoir'),\n",
       " ('have', 'ont'),\n",
       " ('one', 'un'),\n",
       " ('one', 'une'),\n",
       " ('one', 'one'),\n",
       " ('new', 'nouveau')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_fr_train[33:44]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x_i \\in \\mathrm{R}^d$ be the distributed representation of word $i$ in the source language, and $y_i \\in \\mathrm{R}^d$ is the vector representation of its translation. Our purpose is to learn such linear transform $W$ that minimizes euclidian distance between $Wx_i$ and $y_i$ for some subset of word embeddings. Thus we can formulate so-called [Procrustes problem](https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem):\n",
    "\n",
    "$$W^*= \\arg\\min_W \\sum_{i=1}^n\\|Wx_i - y_i\\|_2$$\n",
    "\n",
    "or\n",
    "\n",
    "$$W^*= \\arg\\min_W \\|XW^T - Y\\|_F$$\n",
    "\n",
    "where $\\|\\cdot\\|_F$ denotes Frobenius norm.\n",
    "\n",
    "> **Note:** in second formula, $W$ and $x$ seem to have switched places. This happens because the $X$ matrix is composed of objects $x_i$ in *rows* not *columns*, i.e. it is kind of composed of $x_i^T$. This means that $X \\in \\mathbb{R}^{N \\times D}$, where $N$ is the number of items and $D$ is the embedding dimensionality. The same is true for the $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W^*= \\arg\\min_W \\sum_{i=1}^n\\|Wx_i - y_i\\|_2$ looks like simple multiple linear regression without bias. The `sklearn` allows you to turn off the bias in `LinearRegression` via the `fit_intercept` argument (in fact they simply call bias the intercept). So let's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aout', 0.7475447654724121),\n",
       " ('juin', 0.7295001149177551),\n",
       " ('juillet', 0.7226635813713074),\n",
       " ('septembre', 0.722636342048645),\n",
       " ('mars', 0.7154141068458557),\n",
       " ('octobre', 0.7128994464874268),\n",
       " ('novembre', 0.7042980194091797),\n",
       " ('février', 0.7007734775543213),\n",
       " ('avril', 0.699772298336029),\n",
       " ('janvier', 0.6992713809013367)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "august = mapping.predict(en_emb[\"august\"].reshape(1, -1))\n",
    "fr_emb.most_similar(august)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "mapping = LinearRegression(fit_intercept=False)\n",
    "\n",
    "mapping.fit(X_train, Y_train)\n",
    "Y_pred = mapping.predict(X_test)\n",
    "\n",
    "\n",
    "def precision(pairs, mapped_vectors, topn=1):\n",
    "\n",
    "    assert len(pairs) == len(mapped_vectors)\n",
    "    total = len(pairs)\n",
    "    correct = 0\n",
    "    \n",
    "    for i in range(total):\n",
    "        pair = pairs[i]\n",
    "        predicted_vector = mapped_vectors[i]\n",
    "    \n",
    "        neighbors = fr_emb.most_similar([predicted_vector], topn=topn)\n",
    "        \n",
    "        target_word = pair[1]  # Французское слово\n",
    "        predicted_words = [word for word, _ in neighbors]\n",
    "        \n",
    "        if target_word in predicted_words:\n",
    "            correct += 1\n",
    "    return correct / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1: 0.3403\n",
      "Precision@5: 0.5994\n",
      "Precision@10: 0.6830\n",
      "SVD: \n",
      "Precision@1: 0.3467\n",
      "Precision@5: 0.6454\n",
      "Precision@10: 0.7245\n"
     ]
    }
   ],
   "source": [
    "# Тестовых данных\n",
    "precision_top1 = precision(en_fr_test, Y_pred, 1)\n",
    "precision_top5 = precision(en_fr_test, Y_pred, 5)\n",
    "precision_top10 = precision(en_fr_test, Y_pred, 10)\n",
    "\n",
    "print(f\"Precision@1: {precision_top1:.4f}\")\n",
    "print(f\"Precision@5: {precision_top5:.4f}\")\n",
    "print(f\"Precision@10: {precision_top10:.4f}\")\n",
    "\n",
    "# SVD\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cross_covariance = np.dot(X_train.T, Y_train)\n",
    "U, _, Vt = np.linalg.svd(cross_covariance, full_matrices=False)\n",
    "mapping_svd = np.dot(U, Vt)\n",
    "\n",
    "Y_pred_svd = np.dot(X_test, mapping_svd)\n",
    "precision_svd_top1 = precision(en_fr_test, Y_pred_svd, 1)\n",
    "precision_svd_top5 = precision(en_fr_test, Y_pred_svd, 5)\n",
    "precision_svd_top10 = precision(en_fr_test, Y_pred_svd, 10)\n",
    "\n",
    "print(\"SVD: \")\n",
    "print(f\"Precision@1: {precision_svd_top1:.4f}\")\n",
    "print(f\"Precision@5: {precision_svd_top5:.4f}\")\n",
    "print(f\"Precision@10: {precision_svd_top10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert precision([(\"august\", \"aout\")], august, topn=5) == 1.0\n",
    "assert precision([(\"august\", \"aout\")], august, topn=9) == 1.0\n",
    "assert precision([(\"august\", \"aout\")], august, topn=10) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert precision(en_fr_test[:100], X_test[:100]) == 0.0\n",
    "assert precision(en_fr_test[:100], Y_test[:100]) == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that a self-consistent linear mapping between semantic spaces should be orthogonal. \n",
    "We can restrict transform $W$ to be orthogonal. Then we will solve next problem:\n",
    "\n",
    "$$(W^T)^*= \\arg\\min_{W^T} \\|XW^T - Y\\|_F \\text{, where: } W^TW = I$$\n",
    "\n",
    "$$I \\text{- identity matrix}$$\n",
    "\n",
    "Instead of making yet another regression problem we can find optimal orthogonal transformation using singular value decomposition. It turns out that optimal transformation $W^*$ can be expressed via SVD components:\n",
    "$$X^TY=U\\Sigma V^T\\text{, singular value decompostion}$$\n",
    "$$(W^T)^*=UV^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_covariance = np.dot(X_train.T, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = np.linalg.svd(cross_covariance, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_svd = np.dot(U, Vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "orthogonality_check = np.dot(mapping_svd, mapping_svd.T)\n",
    "identity_diff = orthogonality_check - np.eye(orthogonality_check.shape[0])\n",
    "orthogonality_error = np.max(np.abs(identity_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_svd = np.dot(X_test, mapping_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_svd_top1 = precision(en_fr_test, Y_pred_svd, 1)\n",
    "precision_svd_top5 = precision(en_fr_test, Y_pred_svd, 5)\n",
    "precision_svd_top10 = precision(en_fr_test, Y_pred_svd, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"august\" in en_emb:\n",
    "    august_emb = en_emb[\"august\"]\n",
    "    august_fr_emb = np.dot(august_emb, mapping_svd)\n",
    "    august_neighbors = fr_emb.most_similar([august_fr_emb], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aout', 0.6530280709266663),\n",
       " ('juin', 0.6380628943443298),\n",
       " ('juillet', 0.631451427936554),\n",
       " ('septembre', 0.6301831603050232),\n",
       " ('octobre', 0.6239124536514282),\n",
       " ('mars', 0.6188206672668457),\n",
       " ('août', 0.6144081354141235),\n",
       " ('novembre', 0.6125038862228394),\n",
       " ('fevrier', 0.6092208027839661),\n",
       " ('février', 0.6086474061012268)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_emb.most_similar([np.matmul(en_emb['august'], mapping_svd)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36\n",
      "0.71\n"
     ]
    }
   ],
   "source": [
    "print(precision(en_fr_test[:100], np.matmul(X_test[:100], mapping_svd)))\n",
    "print(precision(en_fr_test[:100], np.matmul(X_test[:100], mapping_svd), 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build our word embeddings-based translator!'\n",
    "'Now let's translate these sentences word-by-word. Before that, however, don't forget to tokenize your sentences. For that you may (or may not) find the `nltk.tokenize.WordPunctTokenizer` to be very useful.'\n",
    "''\n",
    "''\n",
    "'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примеры переводов:\n",
      "--------------------------------------------------\n",
      "EN: Hello, how are you today?\n",
      "FR: Bonjour , comment sont vous hui ?\n",
      "--------------------------------------------------\n",
      "EN: I like to read books about history.\n",
      "FR: Sje veux amener lire livres racontant histoire .\n",
      "--------------------------------------------------\n",
      "EN: The weather is beautiful in Paris.\n",
      "FR: Dans météo est magnifique dans Londre .\n",
      "--------------------------------------------------\n",
      "EN: Can you translate this document for me?\n",
      "FR: Peut vous traduire cette document pour me ?\n",
      "--------------------------------------------------\n",
      "EN: She works as a software engineer.\n",
      "FR: Elle œuvres comme un logiciels ingénieur .\n",
      "--------------------------------------------------\n",
      "\n",
      "Примеры улучшенных переводов:\n",
      "--------------------------------------------------\n",
      "EN: Hello, how are you today?\n",
      "FR: Bonjour , comment sont vous hui ?\n",
      "--------------------------------------------------\n",
      "EN: I like to read books about history.\n",
      "FR: Sje veux amener lire livres racontant histoire .\n",
      "--------------------------------------------------\n",
      "EN: The weather is beautiful in Paris.\n",
      "FR: Dans météo est magnifique dans Londre .\n",
      "--------------------------------------------------\n",
      "EN: Can you translate this document for me?\n",
      "FR: Peut vous traduire cette document pour me ?\n",
      "--------------------------------------------------\n",
      "EN: She works as a software engineer.\n",
      "FR: Elle œuvres comme un logiciels ingénieur .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    :args:\n",
    "        sentence - sentence in English (str)\n",
    "    :returns:\n",
    "        translation - sentence in French (str)\n",
    "\n",
    "    * find english embedding for each word in sentence\n",
    "    * transform english embedding vector\n",
    "    * find nearest french word and replace\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    translated = []\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "    \n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in \".,!?;:\\\"'()[]{}\":\n",
    "            translated.append(token)\n",
    "            continue\n",
    "        token_lower = token.lower()\n",
    "\n",
    "        if token_lower in en_emb:\n",
    "            en_embedding = en_emb[token_lower]\n",
    "            fr_embedding = np.dot(en_embedding, mapping_svd)\n",
    "            most_similar = fr_emb.most_similar([fr_embedding], topn=1)\n",
    "            fr_word = most_similar[0][0]\n",
    "            if token[0].isupper() and len(fr_word) > 0:\n",
    "                fr_word = fr_word[0].upper() + fr_word[1:] if len(fr_word) > 1 else fr_word.upper()\n",
    "                \n",
    "            translated.append(fr_word)\n",
    "        else:\n",
    "            token_variants = [token, token.lower(), token.capitalize(), token.upper()]\n",
    "            found = False\n",
    "            \n",
    "            for variant in token_variants:\n",
    "                if variant in en_emb:\n",
    "                    en_embedding = en_emb[variant]\n",
    "                    fr_embedding = np.dot(en_embedding, mapping_svd)\n",
    "                    most_similar = fr_emb.most_similar([fr_embedding], topn=1)\n",
    "                    fr_word = most_similar[0][0]\n",
    "                    \n",
    "\n",
    "\n",
    "                    if token[0].isupper() and len(fr_word) > 0:\n",
    "                        fr_word = fr_word[0].upper() + fr_word[1:] if len(fr_word) > 1 else fr_word.upper()\n",
    "                        \n",
    "                    translated.append(fr_word)\n",
    "                    found = True\n",
    "                    break\n",
    "            \n",
    "            if not found:\n",
    "                translated.append(token)\n",
    "    \n",
    "    return \" \".join(translated)\n",
    "\n",
    "\n",
    "english_sentences = [\n",
    "    \"Hello, how are you today?\",\n",
    "    \"I like to read books about history.\",\n",
    "    \"The weather is beautiful in Paris.\",\n",
    "    \"Can you translate this document for me?\",\n",
    "    \"She works as a software engineer.\"\n",
    "]\n",
    "\n",
    "print(\"Примеры переводов:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for sentence in english_sentences:\n",
    "    translation = translate(sentence)\n",
    "    print(f\"EN: {sentence}\")\n",
    "    print(f\"FR: {translation}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def translate_improved(sentence):\n",
    "    \"\"\"\n",
    "    Улучшенная версия функции перевода с дополнительной обработкой\n",
    "    \"\"\"\n",
    "    import re\n",
    "    sentence = re.sub(r'http\\S+', '', sentence)\n",
    "    sentence = re.sub(r'@\\S+', '', sentence)\n",
    "    basic_translation = translate(sentence)\n",
    "    return basic_translation\n",
    "\n",
    "print(\"\\nПримеры улучшенных переводов:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for sentence in english_sentences:\n",
    "    translation = translate_improved(sentence)\n",
    "    print(f\"EN: {sentence}\")\n",
    "    print(f\"FR: {translation}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
